
Language models have recently demonstrated impressive abilities in generating varied and persuasive text based on human prompts. However, defining "good" text is challenging due to its subjective and contextual nature. In applications like storytelling, where creativity is essential, or informative text that must be truthful, or code that needs to be executable, the ideal qualities differ widely. Writing a specific loss function to capture these qualities is complex; hence, most language models are trained using a straightforward next-token prediction loss (such as cross-entropy). To address limitations in this approach, metrics like BLEU and ROUGE have been developed to better capture human preferences, yet these metrics are limited, comparing generated text to references using fixed rules. A promising approach, Reinforcement Learning from Human Feedback (RLHF), leverages human feedback directly as a performance measure or as a loss function to enhance the model’s alignment with human values.

### RLHF: Key Steps

Reinforcement Learning from Human Feedback (RLHF) is a multi-step process involving:

1. **Pretraining a Language Model (LM)**: Language models are first pretrained using standard objectives. Initial models, like OpenAI’s InstructGPT, used smaller versions of GPT-3, while Anthropic and DeepMind experimented with larger models up to 280 billion parameters. Though fine-tuning on human-preferred text can improve the model's alignment with specific criteria (such as "helpful, honest, and harmless"), it’s not essential for understanding RLHF.
    
2. **Training a Reward Model (RM)**: The next step in RLHF involves developing a reward model calibrated with human preferences. This reward model aims to convert human preferences into a numerical reward, essential for integrating reinforcement learning algorithms into the RLHF process. The reward model may rank outputs or directly convert rankings into rewards.

Language models for reward modeling can either be fine-tuned versions of existing language 
models or entirely new models trained specifically on preference data. Anthropic, for instance, developed a "preference model pretraining" (PMP) method, which efficiently initializes models from pretrained ones, though there isn’t a universally agreed-upon best model choice for reward models.

The training data for the reward model (RM) is created by sampling prompts from a dataset. Organizations like Anthropic and OpenAI have used data generated by human annotations (e.g., using Amazon Mechanical Turk) and prompts submitted to their APIs. These prompts are passed through an initial language model to create text outputs, which human annotators then rank. Rather than assigning a scalar score to each text, annotators rank the outputs to produce a more regularized dataset.

Different ranking methods can be applied, such as comparing two language model outputs side-by-side using an Elo system, which ranks models and outputs relative to each other. This ranking is then transformed into a scalar reward, making it usable for reinforcement learning algorithms in the next step.

### Fine-tuning with Reinforcement Learning (RL)
Training a language model using reinforcement learning was once seen as nearly impossible due to engineering and algorithmic challenges. However, organizations have successfully employed Proximal Policy Optimization (PPO), a policy-gradient RL algorithm, to fine-tune language models. Because fine-tuning all parameters in large models is costly, some parameters are frozen. Methods like Low-Rank Adaptation (LoRA) are also used to manage the computational load, and the number of parameters to freeze remains an open research area.

In formulating fine-tuning as an RL task, the language model is treated as a policy that generates text in response to prompts. The reward function combines feedback from the preference model and a penalty to limit divergence from the initial model’s behavior. This penalty is often calculated using Kullback–Leibler (KL) divergence, which keeps the model’s output coherent. OpenAI, Anthropic, and DeepMind use a formula combining preference-based rewards and KL divergence adjustments to prevent the model from straying too far from its original text generation.

OpenAI added additional gradients from pretraining data for InstructGPT, showing the reward function could evolve as RLHF research progresses. DeepMind, in contrast, used synchronous advantage actor-critic (A2C) for its Gopher model, introducing an alternative optimization technique. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman). The final reward sent to the RL update rule is 

``` tex
\( r = r_{\theta} - \lambda r_\text{KL} \)
```


Some RLHF systems iterate by updating both the reward model and policy over time, allowing users to rank outputs compared to previous versions, a concept Anthropic describes as "Iterated Online RLHF." This iterative method allows for a more dynamic evolution of the reward and policy models but poses significant research challenges.

## What’s next for RLHF?

Though RLHF has shown promise and gained interest from major AI research labs, it still faces significant challenges. The models, despite improvements, can still generate harmful or inaccurate text, and eliminating this flaw is challenging. In the inherently subjective field of human preferences, it is difficult to define a final goal or an [[endpoint]] for RLHF models.

Deploying RLHF systems is also costly due to the need for human-generated data, with performance dependent on the quality of human annotations. Annotations include human-preferred text samples for fine-tuning (as seen in InstructGPT) and labels of preferred outputs between model-generated texts. Obtaining high-quality, human-generated responses to specific prompts is especially costly, often requiring paid staff. Although the number of preference-labeled samples for RLHF (around 50,000) is manageable, it is still more costly than what academic labs can typically afford. Large-scale general datasets for RLHF, like Anthropic’s, are rare, and smaller datasets focus on specific tasks (e.g., OpenAI’s summarization data). Additionally, human annotators often disagree, introducing variability into training data.

With these constraints, there is vast potential for advancing RLHF by exploring design options, especially in RL optimization. While PPO is the standard, other algorithms could improve RLHF. One costly part of fine-tuning is that each generated text needs evaluation by the reward model, acting as part of the RL environment. To reduce these costly evaluations, offline RL could optimize policy training. Emerging algorithms, such as implicit language Q-learning (ILQL), are suited to this type of optimization. Other trade-offs in the RL process, such as balancing exploration and exploitation, remain underexplored. Investigating these areas could deepen understanding of RLHF and possibly enhance its performance.

---
---
Los modelos de lenguaje han demostrado recientemente habilidades impresionantes para generar texto variado y persuasivo en respuesta a indicaciones humanas. Sin embargo, definir qué es un "buen" texto resulta complicado debido a su naturaleza subjetiva y contextual. En aplicaciones como la narrativa, donde la creatividad es esencial; el texto informativo, que debe ser veraz; o el código, que necesita ser ejecutable, las cualidades ideales varían ampliamente. Escribir una función de pérdida específica que capture estas cualidades es complejo; por ello, la mayoría de los modelos de lenguaje se entrenan usando una simple función de pérdida de predicción del próximo token (como la entropía cruzada). Para abordar las limitaciones de este enfoque, se han desarrollado métricas como BLEU y ROUGE, que capturan mejor las preferencias humanas, aunque estas métricas son limitadas y comparan el texto generado con referencias usando reglas fijas. Un enfoque prometedor, el Aprendizaje por Refuerzo a partir de Retroalimentación Humana (RLHF, por sus siglas en inglés), aprovecha la retroalimentación humana directamente como medida de rendimiento o como función de pérdida para mejorar la alineación del modelo con los valores humanos.

### RLHF: Pasos Clave

El Aprendizaje por Refuerzo a partir de Retroalimentación Humana (RLHF) es un proceso de múltiples pasos que implica:

1. **Preentrenamiento de un Modelo de Lenguaje (LM)**: Los modelos de lenguaje se preentrenan primero usando objetivos estándar. Los modelos iniciales, como InstructGPT de OpenAI, usaban versiones más pequeñas de GPT-3, mientras que Anthropic y DeepMind experimentaron con modelos más grandes de hasta 280 mil millones de parámetros. Aunque el ajuste fino en textos preferidos por humanos puede mejorar la alineación del modelo con criterios específicos (como "útil, honesto e inofensivo"), no es esencial para comprender el RLHF.

2. **Entrenamiento de un Modelo de Recompensa (RM)**: El siguiente paso en RLHF implica desarrollar un modelo de recompensa calibrado con las preferencias humanas. Este modelo de recompensa tiene como objetivo convertir las preferencias humanas en una recompensa numérica, esencial para integrar algoritmos de aprendizaje por refuerzo en el proceso de RLHF. El modelo de recompensa puede clasificar las salidas o convertir directamente las clasificaciones en recompensas.

Los modelos de lenguaje para modelar recompensas pueden ser versiones ajustadas de modelos de lenguaje existentes o modelos completamente nuevos entrenados específicamente con datos de preferencias. Anthropic, por ejemplo, desarrolló un método llamado "preentrenamiento del modelo de preferencias" (PMP), que inicializa eficientemente los modelos a partir de modelos ya preentrenados, aunque no existe un consenso universal sobre el mejor modelo para las recompensas.

Los datos de entrenamiento para el modelo de recompensa (RM) se crean tomando muestras de indicaciones de un conjunto de datos. Organizaciones como Anthropic y OpenAI han usado datos generados a partir de anotaciones humanas (por ejemplo, utilizando Amazon Mechanical Turk) e indicaciones enviadas a sus APIs. Estas indicaciones se procesan con un modelo de lenguaje inicial para crear salidas de texto que luego son clasificadas por anotadores humanos. En lugar de asignar un puntaje a cada texto, los anotadores clasifican las salidas para producir un conjunto de datos más regularizado.

Se pueden aplicar diferentes métodos de clasificación, como comparar dos salidas de modelo de lenguaje lado a lado utilizando un sistema Elo, que clasifica modelos y salidas en relación entre sí. Esta clasificación se transforma luego en una recompensa escalar, haciéndola utilizable para algoritmos de aprendizaje por refuerzo en el siguiente paso.

### Ajuste Fino con Aprendizaje por Refuerzo (RL)

Entrenar un modelo de lenguaje utilizando aprendizaje por refuerzo fue considerado casi imposible debido a desafíos de ingeniería y algorítmicos. Sin embargo, algunas organizaciones han empleado con éxito el algoritmo Proximal Policy Optimization (PPO), un algoritmo de aprendizaje por refuerzo basado en gradientes de política, para ajustar los modelos de lenguaje. Dado que ajustar todos los parámetros en modelos grandes es costoso, algunos parámetros se congelan. Métodos como la Adaptación de Bajo Rango (LoRA) también se usan para gestionar la carga computacional, y el número de parámetros a congelar sigue siendo un área de investigación abierta.

Al formular el ajuste fino como una tarea de RL, el modelo de lenguaje se trata como una política que genera texto en respuesta a indicaciones. La función de recompensa combina retroalimentación del modelo de preferencias y una penalización para limitar la divergencia respecto al comportamiento inicial del modelo. Esta penalización a menudo se calcula usando la divergencia de Kullback–Leibler (KL), que mantiene la coherencia de las salidas del modelo. OpenAI, Anthropic y DeepMind usan una fórmula que combina recompensas basadas en preferencias y ajustes de divergencia KL para evitar que el modelo se desvíe demasiado de su generación de texto original.

OpenAI agregó gradientes adicionales de datos de preentrenamiento para InstructGPT, mostrando que la función de recompensa podría evolucionar a medida que avanza la investigación en RLHF. DeepMind, en contraste, usó el actor-crítico de ventaja sincrónica (A2C) para su modelo Gopher, introduciendo una técnica de optimización alternativa. En la práctica, la divergencia KL se aproxima mediante muestreo de ambas distribuciones (explicado por John Schulman).

Algunos sistemas de RLHF iteran actualizando tanto el modelo de recompensa como la política a lo largo del tiempo, permitiendo a los usuarios clasificar las salidas en comparación con versiones anteriores, un concepto que Anthropic describe como "RLHF Online Iterado". Este método iterativo permite una evolución más dinámica de los modelos de recompensa y política, aunque plantea desafíos significativos de investigación.

## ¿Qué sigue para RLHF?

Aunque RLHF ha demostrado potencial y ha ganado interés en los principales laboratorios de investigación en IA, aún enfrenta desafíos importantes. Los modelos, a pesar de las mejoras, aún pueden generar texto dañino o inexacto, y eliminar este problema es complejo. En el campo inherentemente subjetivo de las preferencias humanas, es difícil definir un objetivo final o un punto de cierre para los modelos de RLHF.

Implementar sistemas de RLHF también es costoso debido a la necesidad de datos generados por humanos, con el rendimiento dependiendo de la calidad de las anotaciones humanas. Estas anotaciones incluyen muestras de texto preferidas por humanos para el ajuste fino (como en InstructGPT) y etiquetas de salidas preferidas entre textos generados por el modelo. Obtener respuestas de alta calidad generadas por humanos para indicaciones específicas es especialmente costoso, lo que a menudo requiere personal remunerado. Aunque el número de muestras con etiquetas de preferencia para RLHF (alrededor de 50,000) es manejable, sigue siendo más costoso de lo que los laboratorios académicos típicamente pueden pagar. Los conjuntos de datos generales a gran escala para RLHF, como el de Anthropic, son raros, y los conjuntos de datos más pequeños se centran en tareas específicas (por ejemplo, los datos de resumen de OpenAI). Además, los anotadores humanos suelen estar en desacuerdo, lo que introduce variabilidad en los datos de entrenamiento.

Con estas limitaciones, existe un amplio potencial para avanzar en RLHF explorando opciones de diseño, especialmente en la optimización de RL. Aunque PPO es el estándar, otros algoritmos podrían mejorar el RLHF. Una parte costosa del ajuste fino es que cada texto generado necesita ser evaluado por el modelo de recompensa, que actúa como parte del entorno de RL. Para reducir estas evaluaciones costosas, RL offline podría optimizar el entrenamiento de la política. Algoritmos emergentes, como el aprendizaje Q implícito de lenguaje (ILQL), son adecuados para este tipo de optimización. Otros compromisos en el proceso de RL, como el equilibrio entre exploración y explotación, siguen poco explorados. Investigar estas áreas podría profundizar el entendimiento de RLHF y posiblemente mejorar su rendimiento.