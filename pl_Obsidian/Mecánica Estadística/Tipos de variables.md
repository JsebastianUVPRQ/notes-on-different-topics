Here is the extracted text from the provided image:

---

- **Discrete random variables:** Discrete random variables can take on a discrete spectrum of values. We can denote an arbitrary value in this discrete spectrum as $j$. Each value is associated with a probability $p_j$ to obtain that value from the experiment. These $p_j$ satisfy the normalization requirement

$$
1 = \sum_{j} p_j,
$$

where the sum is over all possible values of$j$. We can compute the average of a function $f(j)$ of the random variable by calculating the probability weighted sum of the function over all possible values of the random variable. Namely, denoting the average of $f(j)$ as $\langle f(j) \rangle$ we have

$$
\langle f(j) \rangle = \sum_{j} f(j) p_j.
$$

Two important applications of Eq.(3) are to calculating the mean and the variance of a random variable:

$$
\langle j \rangle = \sum_{j} j p_j \quad \text{and} \quad \sigma_j^2 = \langle j^2 \rangle - \langle j \rangle^2.
$$

- **Continuous random variables:** Continuous random variables can take on a continuous spectrum of values. We can denote an arbitrary value in this continuous spectrum as $x$. Unlike in the discrete case, we cannot define the probability to be at a particular $x$, but we can define the probability density as a function of $x$. We label this probability density as $p(x)$ and define it as

  **Probability density $p(x)$ (defined):** If $p(x)$ is the probability density for a continuous random variable $x$, then, for $\Delta x$ sufficiently small, the probability that we get a value of $x$

---