Review the metrics to the left of the confusion matrix, which include: 

- **Accuracy**: The ratio of correct predictions (true positives + true negatives) to the total number of predictions. In other words, what proportion of diabetes predictions did the model get right? 
    
- **Precision**: The fraction of positive cases correctly identified (the number of true positives divided by the number of true positives plus false positives). In other words, out of all the patients that the model predicted as having diabetes, how many are actually diabetic? 
    
- **Recall**: The fraction of the cases classified as positive that are actually positive (the number of true positives divided by the number of true positives plus false negatives). In other words, out of all the patients who actually have diabetes, how many did the model identify? 
    
- **F1 Score**: An overall metric that essentially combines precision and recall. 
    
- _We'll return to_ _**AUC**_ _later_.